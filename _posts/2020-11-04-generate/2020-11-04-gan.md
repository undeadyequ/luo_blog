---
layout: post
title:  "Gan"
date:   2020-11-04 23:57:05 +0900
categories: jekyll update
---
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

###  Core
  - Value Function

$$ \min_{G}\max_{D}V(D,G) = \mathbb{E}_{x \sim p_{data}(x)} [\log{D(x)}] + \mathbb{E}_{x \sim p_{z}(z)}[\log(1-D(G(Z))]  $$

  - Train process

**for** interations do  
&nbsp; **for** key steps do  
&nbsp; &nbsp; ascending its stochastic gradient  
&nbsp; **end** for  
&nbsp; &nbsp; descending its stochastic gradient  
**end** for  

  - Converting generative problem to Discriminate problem which is easier.
## Proof of optimal
  - The global minimum of the virtual training criterion:
      $$ C(G) = \max_{D}V(G,D) $$
    is acheived if and only if
      $$ p_{data} = p_{g} $$
  - Got optimal G if and only if D get maximum:
      $$ -\log{4} $$
  - p_G = p_data equals C(G) = -log4

  - why two x merged to same x, it shouldn't be samiliar witout any guarantee.
    - Radon-Nikodym Theorem
    - Law of the unconscious statistician

![hi](/assets/gan/optimal.png)

## convergence

## related paper
[Self-Attention Generative Adversarial Networks](https://arxiv.org/pdf/1805.08318.pdf)


## reference
[proof of gan](https://srome.github.io/An-Annotated-Proof-of-Generative-Adversarial-Networks-with-Implementation-Notes/)

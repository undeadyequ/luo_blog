<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>


###  Core
  - Assume hidden states Z are obey general Norm distribution, and one new sample X_1 can be generated from Given Z_1, derived from X_1 itself. (Assume to any distribution is ok due to Formula1)
  - Z_1 conditioned on X_1 or P(Z/X) obey the sample-based norm of the general norm, because 1. Make each sample different, 2. Make Diversity 3. Avoid Overfitting.
    - Caution: P(Z/X) obey norm not P(Z)

## CVAE
  - 




  - Thinking
    - Each Data is generated conditioned on several Hidden states(Z), which are sampled from semi-normal distribution. p(z/x) ~ N(0,1)
    - the Z of similarly data should be near.
    - Sampling from p(z/x) guarantee the diversity, and KL(p(x),p(x/z)) itself guarantee the Correctness. (Compare with Gan, Discriminator guarantee the Correctness, and )


    - The integration showed below is intractable, since z are not clear defined. ?
    - z is Normal
$$
p_{\theta}(x) = \int p_{\theta}(z) * p_{\theta}(x|z)dz
$$

  - Value Function
    - Make postier equal to prior ?

$$
\begin{eqnarray}
\log{p_{\theta}(x)} &=& \mathbb{E}_{z\sim q_{\phi}(z|x)} [ \log{p_{\theta}(x)} ] \\

&=& \mathbb{E}_{z} [ log{
  \frac{p_{\theta}(x|z)*p_{\theta}(z)}{p_{\theta}(z|x)}
  *
  \frac{q_{\phi}(z|x)}{q_{\phi}(z|x)} } ]  \\

&=& \mathbb{E}_{z}[\log{p_{\theta}(x|z)}] -
D_{KL}(q_{\phi}(z|x) || p_{\theta}(z)) +
D_{KL}(q_{\phi}(z|x) || p_{\theta}(z|x))  
\end{eqnarray}
$$


$$

KL(N(\mu, \sigma), N(0, 1)) = \log \frac{1}{\sigma} + \frac{\sigma^2 + \mu^2}{2} - \frac{1}{2}

$$

  - Train process
    - z is meaningful?




## related papers

[Improved Variational Inference with Inverse Autoregressive Flow](https://papers.nips.cc/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf)
- high-dimensional latent spaces

[变分自编码器VAE：原来是这么一回事 | 附开源代码](https://zhuanlan.zhihu.com/p/34998569)
## question
  - Interprtable of z
  - Delichelt VAE

---
layout: post
title:  "c_emo_tts"
---

<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>


1. reference audio => Emphasized word extraction (w1)
  * w1 = f(ref_audio) (11/28)

### Semi-supervising token by emotional recognizer
- Pre-training emo detector
- Add additional loss of token emo_classifier and pretrained labeled classifier
  -

## Can't set emotion label
  - Visual Expression Label
  - Adversarial Factorization
  - Anchor Tokens

## Can not Learn saliently local prosodic style clearly

### Semantic-Aware Expressive TTS
1. reference audio => Emphasized word extraction (w1) (11/27~11/28)
   * Forced alignment + compare F_0/Power/Duration
     * w1 = f(ref_audio) <= Forced alignment method
       1. monophone+triphone level HMM+GMM based on ASR alignment(Kaldi)
          - phone level reduced align error
       2. DTW(Dynamic Time Warping) after TTS alignment
       3. ?
  [hmm-gmm](https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196)
     * compare prosodic feature of each word with it in database
   * **Forced alignment with emphasis weight in Normal**
     * LR-HSMM
   * contrastive Learning
     *


$$ \prod_{s=1}^{2} N(\mu_i^{(s)} + w_t * b_i^{(s)}, \Sigma_i^{(s)})   $$

      - supervised train:
      1. Selecting Gaussian distribution by speech/transcription
      2. Cluster adaptive training for estimate emphasis by speech/transcription/emphasis

   * Highest Pitch
      - Con: Duration, power are ignored

2. extract semantic role and sentiment role of w1 (11/29)

   `w2_sem = sem_sim(sem(w1, ref_trans), input_text)`  
   `w2_sent = sent_sim(sent(w1, ref_trans), input_text)`  
   `w2 = w2_sem and w2_sent`


3. Embedding w2
  - prosodic{frame(w2)} ~ prosodic{frame(w1)}

4. Experiment (12/10)
  - Multiple ref_audio
    - semantic/sentiment similar transcription with input
    - ...  no-similar
  - Input Text
  - GST VS semantic GST
  - Eval
    - Natural
    - Expressive
    - **Semantic similarity(SS)**

Given: Input

| Model     | ref_audio(trans)   | SS | sytle |
| :------------- | :------------|:--- |:--- |
| Semantic_GST   | sim<br>non_ssim<br>non_ssim<br>  | 4.5 | |
| GST   | sim<br>non_ssim<br>non_ssim      | 3<br>2<br>2 | |



  - Finding difference by Contrastive learning <1>
    - [Learn_Contrastive_learning](https://zhuanlan.zhihu.com/p/141141365https://zhuanlan.zhihu.com/p/141141365)
  - Finding Word Level empahsisze<2>
    - [related_paper](https://drive.google.com/file/d/0BwCq1DWnNN4NWlhuQ21kSzU1VTQ/view)
- Can not Match Text with proper Prosodic Style
  - Generate proper token from context and audio
- Can not representation emotion correctly[1]
  - GER: Generative Emotion Representation Model with Event Appraisal Theory [2]
    - [related_2](https://www.affective-science.org/pubs/2001/FBGross2001.pdf)

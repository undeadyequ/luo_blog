## Abstract.
This paper proposed AutoEncoder Tacotron model to fine-grained control the emotional speech by both specific emotion and intuitive prosody.ã€€Compare to previous research, our proposed model can fine-grained control utterance-level prosody of generated speech based on specific emotion. To alleviate the lack of emotional speech data for Emotional TTS model training, we also propose an efficient approach to collect relative clarity emotional speech from audio-book speech data. The Experiment shows our proposed model could generate angry speech data with 0.66 accuracy and fine-grained controlling the angry speech by mean, std, range of pitch and energy and also harmonic.

## Introduction
- What is Emo TTS
- why research Emo TTS
- The Difficulty of Emo TTS
  - The Diversity of emo, and current research doesn't match
  - Need Large Scale high quality

- Core Contribution
  - Can fine-grained control speech by prosody under given emotion labels
  - efficiently extract emo speech

## Related Research
- Expressive TTS
  - Can not control by specific emotion

- Emotional TTS
  - Can not fine-grained training model

## Data
IEMOCAP and Blizzard2013 are both applied as training data, in which the first one is used to train partially the Labeled Auto-encoder Network, and the latter one is used to train overall model with Encoder of Auto-encoder freezed.


### IEMOCAP
IEMOCAP has 12 hours of utterance-level audio-transcription data, recording from emotional dialogue of 5 male and 5 female in both acting and improvising way. 1. Emotion data distribution and Choose 4 (Happy, Angry, Sad, Neu) >>

We randomly split it to 80% and 20% for training and testing the Auto-Encoder Network.

### Blizzard2013
Blizzard2013 is a large-scale audio-book data recorded by Catherine Byers in emotive story-telling style. Collecting distinctly emotion-specific audio play a crucial position in training emotional TTS model. We propose 3 steps to efficiently collect this at low cost.


## Technical Overview
Our proposed model contains 3 parts, 1) Emotional Feature Extractor extracts utterance-level emotional
feature from text and audio. 2) Labeled Auto-encoder encodes extracted emo-features
into specific emotion representation, and then decodes back to Prosody Feature for fine-grained
Controlling. the specific emotion representation is guaranteed by labeled speech dataset, hence called
Labeled Auto-encoder. To be clear, the emotional feature mentioned here includes prosody linguist feature.


### Emotional Feature extractor
Emotional Feature includes Prosody Feature and Textual Feature.

Regarding to prosody feature,  A[1] shows Mean and Standard Deviation of Pitch, Energy and harmonically is highly related to speech emotion, appended with
the range of both Pitch and Energy for further prosodic controlling, addictively we extract 8 prosody features.

In text part, we apply TF-IDF as textural feature, and IEMOCAP Corpus is emoployed as TF-IDF corpus.

2)

### Labeled Auto-encoder
The Labeled Auto-encoder includes Encoder and Decoder parts. The Encoder implemented with LSTM-512 and the Decoder implemented with DNN_3_512. Loss Function is generated by below. >> 


### Auto-Encoder Tacotron
Tacotron takes Contextual Vector, output from attention model and Prosody, generated by the auto-encoder, as decoder input to generate mel-spectrum. The Loss Function is computed by below: Fomular (1) >>

Architect Pic(2)

#### Collect emotional Blizzard2013
1. Select character dialogue audio. Observably, in audio-book data, emotional audio is mainly distributed in character dialogue, which can be simply located by the text surround with single or double question mark.
2. Filter emotional audio from step1 by pre-trained emotional recognizer. Dialogue audio not necessarily be emotional, we applied pre-trained emotion recognizer(The encoder part of autoEncoder) to extract candidate emotional speech by confidence threshold with 0.8.
3. Filter distinctly emotional audio by coarse validation. Due to adaptive and accuracy problem in emotion recognization model, we further validate the extracted the candidate data extracted in step2. Finally, we obtain the distinctively angry and neutral data from candidate data, hence we used this 2 dataset as training data in training Auto-Encoder Tacotron model.


### Training Process
The model is trained by 2 steps.
In 1st training, We pre-train autoEncoder model with labeled IEMOCAP data under additive loss of the cross entropy of emotion and MSE of Prosody.
In 2nd training, The Overall model is trained with Blizzard2013 under the freezed Encoder in pre-trained autoEncoder. To alleviate adaptive problem, the IECMOAP and Blizzard2013 are normalized respectively.


## Experiment
We choose 130 text from the test part of Emotional Blizzard2013. The character number of text for text is range from 10 to 100.
Our proposed model can control the generated speech by 1) Emotion Label 2) Fine-grained way by both emotional Label and prosody.

### Control by Emotion
Emotion Vector is directly applied to generate corresponding emotion speech. Emotion Vector [0, 1] is for angry and [1, 0] for neutral.
Finally, we generated 130 angry and 130 neutral audio and then create 130 pairs by each paired under the same transcription. Each experimenter is ask to choose the angry audio from the 2 audio, the result shows our model is 66%.

### Fine grained control by Emotion And Prosody
Based on specific emotion vector, our proposed mode could also fine-grained controlling the utterance-level prosody quantitatively. We randomly choose 10 text as text input, mentioned in last section, and fine-grained controlling by input angry vector and adding 5-stage bias in each prosody. To evaluate, we extract the prosody of fine-grained generated audio and shows result in table2 . >>

- Angry * Energy Mean/Std/Range ...
- NEU * Energy Mean/Std/Range ...


## Conclusion
In this paper, we proposed a method to fine-grained emotional speech by both specific emotion and intuitive prosody. In implement, we proposed Labeled Auto-Encoder Tacotron, in which the Labeled Auto-Encoder provide emotion vector and prosody for fine-grained controlling. To alleviate the lack of emotional speech data in Emotional TTS task, we also propose an efficient way to collect clarity emotional speech from audio-book speech data. From experiment result, our model could generate angry speech data with 0.66 accuracy and fine-grained controlling the angry speech by mean, std, range of pitch and energy and also harmonic. Our future works is to collect more emotional speech for training by alleviating the adaptive problem and improving the accuracy of the Emotional recognization model.  
